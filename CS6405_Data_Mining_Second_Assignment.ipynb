{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vqFl23ED4SvM"
      },
      "source": [
        "# Submission from Martin King, 122108604.\n",
        "\n",
        "I have written comments in markdown in some parts of this Jupyter notebook. These comments start with \"My comments:\" in red colour."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8WfrCFmLHxYu"
      },
      "source": [
        "# CS3033/CS6405 - Data Mining - Second Assignment\n",
        "\n",
        "### Submission\n",
        "\n",
        "This assignment is **due on 07/04/22 at 23:59**. You should submit a single .ipnyb file with your python code and analysis electronically via Canvas.\n",
        "Please note that this assignment will account for 25 Marks of your module grade.\n",
        "\n",
        "### Declaration\n",
        "\n",
        "By submitting this assignment. I agree to the following:\n",
        "\n",
        "<font color=\"red\">“I have read and understand the UCC academic policy on plagiarism, and agree to the requirements set out thereby in relation to plagiarism and referencing. I confirm that I have referenced and acknowledged properly all sources used in the preparation of this assignment.\n",
        "I declare that this assignment is entirely my own work based on my personal study. I further declare that I have not engaged the services of another to either assist me in, or complete this assignment”</font>\n",
        "\n",
        "### Objective\n",
        "\n",
        "The Boolean satisfiability (SAT) problem consists in determining whether a Boolean formula F is satisfiable or not. F is represented by a pair (X, C), where X is a set of Boolean variables and C is a set of clauses in Conjunctive Normal Form (CNF). Each clause is a disjunction of literals (a variable or its negation). This problem is one of the most widely studied combinatorial problems in computer science. It is the classic NP-complete problem. Over the past number of decades, a significant amount of research work has focused on solving SAT problems with both complete and incomplete solvers.\n",
        "\n",
        "One of the most successful approaches is an algorithm portfolio, where a solver is selected among a set of candidates depending on the problem type. Your task is to create a classifier that takes as input the SAT instance's features and identifies the class.\n",
        "\n",
        "In this project, we represent SAT problems with a vector of 327 features with general information about the problem, e.g., number of variables, number of clauses, the fraction of horn clauses in the problem, etc. There is no need to understand the features to be able to complete the assignment.\n",
        "\n",
        "\n",
        "The original dataset is available at:\n",
        "https://github.com/bprovanbessell/SATfeatPy/blob/main/features_csv/all_features.csv\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oav9G1WSJ1nH"
      },
      "source": [
        "## Data Preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 386
        },
        "id": "DE0kM0QsJ1En",
        "outputId": "1e33c3da-3e3c-4f05-a5b6-bc2ffdae9f43"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>c</th>\n",
              "      <th>v</th>\n",
              "      <th>clauses_vars_ratio</th>\n",
              "      <th>vars_clauses_ratio</th>\n",
              "      <th>vcg_var_mean</th>\n",
              "      <th>vcg_var_coeff</th>\n",
              "      <th>vcg_var_min</th>\n",
              "      <th>vcg_var_max</th>\n",
              "      <th>vcg_var_entropy</th>\n",
              "      <th>vcg_clause_mean</th>\n",
              "      <th>...</th>\n",
              "      <th>rwh_0_max</th>\n",
              "      <th>rwh_1_mean</th>\n",
              "      <th>rwh_1_coeff</th>\n",
              "      <th>rwh_1_min</th>\n",
              "      <th>rwh_1_max</th>\n",
              "      <th>rwh_2_mean</th>\n",
              "      <th>rwh_2_coeff</th>\n",
              "      <th>rwh_2_min</th>\n",
              "      <th>rwh_2_max</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>608</td>\n",
              "      <td>71</td>\n",
              "      <td>8.563380</td>\n",
              "      <td>0.116776</td>\n",
              "      <td>0.045172</td>\n",
              "      <td>0.173688</td>\n",
              "      <td>0.029605</td>\n",
              "      <td>0.060855</td>\n",
              "      <td>2.802758</td>\n",
              "      <td>0.045172</td>\n",
              "      <td>...</td>\n",
              "      <td>5078250.0</td>\n",
              "      <td>1056.695041</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>2.981935e-09</td>\n",
              "      <td>2113.390083</td>\n",
              "      <td>1081.900778</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.302080e-29</td>\n",
              "      <td>2163.801556</td>\n",
              "      <td>matching</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>615</td>\n",
              "      <td>70</td>\n",
              "      <td>8.785714</td>\n",
              "      <td>0.113821</td>\n",
              "      <td>0.049617</td>\n",
              "      <td>0.168633</td>\n",
              "      <td>0.032520</td>\n",
              "      <td>0.069919</td>\n",
              "      <td>2.607264</td>\n",
              "      <td>0.049617</td>\n",
              "      <td>...</td>\n",
              "      <td>5469376.0</td>\n",
              "      <td>1207.488426</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>6.927306e-28</td>\n",
              "      <td>2414.976852</td>\n",
              "      <td>1186.623627</td>\n",
              "      <td>1.0</td>\n",
              "      <td>3.491123e-120</td>\n",
              "      <td>2373.247255</td>\n",
              "      <td>matching</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>926</td>\n",
              "      <td>105</td>\n",
              "      <td>8.819048</td>\n",
              "      <td>0.113391</td>\n",
              "      <td>0.033385</td>\n",
              "      <td>0.186444</td>\n",
              "      <td>0.017279</td>\n",
              "      <td>0.047516</td>\n",
              "      <td>3.022879</td>\n",
              "      <td>0.033385</td>\n",
              "      <td>...</td>\n",
              "      <td>4297025.0</td>\n",
              "      <td>441.327046</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.194627e-76</td>\n",
              "      <td>882.654092</td>\n",
              "      <td>474.697562</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>949.395124</td>\n",
              "      <td>matching</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>603</td>\n",
              "      <td>70</td>\n",
              "      <td>8.614286</td>\n",
              "      <td>0.116086</td>\n",
              "      <td>0.049799</td>\n",
              "      <td>0.133441</td>\n",
              "      <td>0.033167</td>\n",
              "      <td>0.063018</td>\n",
              "      <td>2.688342</td>\n",
              "      <td>0.049799</td>\n",
              "      <td>...</td>\n",
              "      <td>6640651.0</td>\n",
              "      <td>1181.583331</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>2.437278e-30</td>\n",
              "      <td>2363.166661</td>\n",
              "      <td>1149.059132</td>\n",
              "      <td>1.0</td>\n",
              "      <td>4.670090e-147</td>\n",
              "      <td>2298.118264</td>\n",
              "      <td>matching</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>228</td>\n",
              "      <td>43</td>\n",
              "      <td>5.302326</td>\n",
              "      <td>0.188596</td>\n",
              "      <td>0.067319</td>\n",
              "      <td>0.162581</td>\n",
              "      <td>0.048246</td>\n",
              "      <td>0.087719</td>\n",
              "      <td>2.203308</td>\n",
              "      <td>0.067319</td>\n",
              "      <td>...</td>\n",
              "      <td>2437500.0</td>\n",
              "      <td>1091.423921</td>\n",
              "      <td>0.999966</td>\n",
              "      <td>3.723599e-02</td>\n",
              "      <td>2182.810606</td>\n",
              "      <td>1296.888087</td>\n",
              "      <td>1.0</td>\n",
              "      <td>6.307424e-06</td>\n",
              "      <td>2593.776167</td>\n",
              "      <td>matching</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 328 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     c    v  clauses_vars_ratio  vars_clauses_ratio  vcg_var_mean  \\\n",
              "0  608   71            8.563380            0.116776      0.045172   \n",
              "1  615   70            8.785714            0.113821      0.049617   \n",
              "2  926  105            8.819048            0.113391      0.033385   \n",
              "3  603   70            8.614286            0.116086      0.049799   \n",
              "4  228   43            5.302326            0.188596      0.067319   \n",
              "\n",
              "   vcg_var_coeff  vcg_var_min  vcg_var_max  vcg_var_entropy  vcg_clause_mean  \\\n",
              "0       0.173688     0.029605     0.060855         2.802758         0.045172   \n",
              "1       0.168633     0.032520     0.069919         2.607264         0.049617   \n",
              "2       0.186444     0.017279     0.047516         3.022879         0.033385   \n",
              "3       0.133441     0.033167     0.063018         2.688342         0.049799   \n",
              "4       0.162581     0.048246     0.087719         2.203308         0.067319   \n",
              "\n",
              "   ...  rwh_0_max   rwh_1_mean  rwh_1_coeff     rwh_1_min    rwh_1_max  \\\n",
              "0  ...  5078250.0  1056.695041     1.000000  2.981935e-09  2113.390083   \n",
              "1  ...  5469376.0  1207.488426     1.000000  6.927306e-28  2414.976852   \n",
              "2  ...  4297025.0   441.327046     1.000000  1.194627e-76   882.654092   \n",
              "3  ...  6640651.0  1181.583331     1.000000  2.437278e-30  2363.166661   \n",
              "4  ...  2437500.0  1091.423921     0.999966  3.723599e-02  2182.810606   \n",
              "\n",
              "    rwh_2_mean  rwh_2_coeff      rwh_2_min    rwh_2_max    target  \n",
              "0  1081.900778          1.0   1.302080e-29  2163.801556  matching  \n",
              "1  1186.623627          1.0  3.491123e-120  2373.247255  matching  \n",
              "2   474.697562          1.0   0.000000e+00   949.395124  matching  \n",
              "3  1149.059132          1.0  4.670090e-147  2298.118264  matching  \n",
              "4  1296.888087          1.0   6.307424e-06  2593.776167  matching  \n",
              "\n",
              "[5 rows x 328 columns]"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(\"https://raw.githubusercontent.com/andvise/DataAnalyticsDatasets/main/train_dataset.csv\", index_col=0)\n",
        "df.head()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N8MCvTYTKw4Q",
        "outputId": "ac37a571-ed25-47f8-d144-99fe3500ab37"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tseitin           298\n",
              "dominating        294\n",
              "cliquecoloring    268\n",
              "php               266\n",
              "subsetcard        263\n",
              "op                201\n",
              "tiling            120\n",
              "5clique           108\n",
              "3color            104\n",
              "matching          102\n",
              "5color             98\n",
              "4color             98\n",
              "3clique            98\n",
              "4clique            94\n",
              "Name: target, dtype: int64"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Label or target variable\n",
        "df['target'].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MTvkBPQvITf-"
      },
      "source": [
        "# Tasks\n",
        "\n",
        "## Basic models and evaluation (5 Marks)\n",
        "\n",
        "Using Scikit-learn, train and evaluate a decision tree classifier using 70% of the dataset from training and 30% for testing. For this part of the project, we are not interested in optimising the parameters; we just want to get an idea of the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zl0VXO0YH1nG",
        "outputId": "27ebe003-d608-4944-a156-34783621fdc0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CV Train accuracy:  0.9733394202236931\n",
            "CV Test accuracy:  0.9447413793103449\n",
            "Pred. Train accuracy:  0.9994075829383886\n",
            "Pred. Test accuracy:  0.9737569060773481\n"
          ]
        }
      ],
      "source": [
        "# YOUR CODE HERE\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#from sklearn.preprocessing import LabelEncoder\n",
        "#label_encoder = LabelEncoder()\n",
        "#label_encoder.fit(df['target'])\n",
        "#df['target'] = label_encoder.transform(df['target'])\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "X = df.iloc[:, :-1]\n",
        "y = df['target']\n",
        "\n",
        "# Clean data.\n",
        "X = X.fillna(0)\n",
        "X = X.replace([np.inf, -np.inf], 0)\n",
        "\n",
        "train_X, test_X, train_y, test_y = train_test_split(X, y, train_size=0.7, random_state=122108)\n",
        "\n",
        "# Scale the features.\n",
        "#scaler = StandardScaler()\n",
        "#train_X = scaler.fit_transform(train_X)\n",
        "#test_X = scaler.transform(test_X)\n",
        "\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "classifierDT = Pipeline([\n",
        "    # ('dr', PCA(327)),\n",
        "    (\"predictor\", DecisionTreeClassifier())\n",
        "])\n",
        "\n",
        "classifierDT.fit(train_X, train_y)\n",
        "\n",
        "from sklearn.model_selection import cross_val_score\n",
        "train_acc = np.mean(cross_val_score(classifierDT, train_X, train_y, scoring=\"accuracy\"))\n",
        "test_acc = np.mean(cross_val_score(classifierDT, test_X, test_y, scoring=\"accuracy\"))\n",
        "\n",
        "print(\"CV Train accuracy: \", train_acc)\n",
        "print(\"CV Test accuracy: \", test_acc)\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "predictions = classifierDT.predict(train_X)\n",
        "train_acc = accuracy_score(predictions, train_y)\n",
        "predictions = classifierDT.predict(test_X)\n",
        "test_acc = accuracy_score(predictions, test_y)\n",
        "\n",
        "print(\"Pred. Train accuracy: \", train_acc)\n",
        "print(\"Pred. Test accuracy: \", test_acc)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AdnPLc-I4SvX"
      },
      "source": [
        "<font color=\"red\">**My comment:**</font> The predicted training and test accuracies are at 99.94% and 98.48% respectively. These are quite high values already. In the following 2 sections, especially the final section, an attempt is made to improve the accuracy even further."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zADpr0f8IcGL"
      },
      "source": [
        "## Robust evaluation (10 Marks)\n",
        "\n",
        "In this section, we are interested in more rigorous techniques by implementing more sophisticated methods, for instance:\n",
        "* Hold-out and cross-validation.\n",
        "* Hyper-parameter tuning.\n",
        "* Feature reduction.\n",
        "* Feature selection.\n",
        "* Feature normalisation.\n",
        "\n",
        "Your report should provide concrete information about your reasoning; everything should be well-explained.\n",
        "\n",
        "The key to geting good marks is to show that you evaluated different methods and that you correctly selected the configuration."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ybtiqBNA4SvY"
      },
      "source": [
        "<font color=\"red\">**My comment:**</font> I use GridSearchCV to find optimal values for PCA components to retain, maximum depths in tree, minimum number of samples in a node to split, and minimum number of samples must be kept in a leaf."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tvBZH6ilInsA",
        "outputId": "0331f7ba-cdc8-482d-ac78-f97c121458ff"
      },
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[15], line 23\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodel_selection\u001b[39;00m \u001b[39mimport\u001b[39;00m GridSearchCV\n\u001b[1;32m     22\u001b[0m dt_gs \u001b[39m=\u001b[39m GridSearchCV(classifierDT, dt_param_grid, scoring\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39maccuracy\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 23\u001b[0m dt_gs\u001b[39m.\u001b[39;49mfit(train_X, train_y)\n\u001b[1;32m     25\u001b[0m dt_gs\u001b[39m.\u001b[39mbest_params_, dt_gs\u001b[39m.\u001b[39mbest_score_\n",
            "File \u001b[0;32m~/opt/anaconda3/envs/tensorflow/lib/python3.9/site-packages/sklearn/model_selection/_search.py:874\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    868\u001b[0m     results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_results(\n\u001b[1;32m    869\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[1;32m    870\u001b[0m     )\n\u001b[1;32m    872\u001b[0m     \u001b[39mreturn\u001b[39;00m results\n\u001b[0;32m--> 874\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_search(evaluate_candidates)\n\u001b[1;32m    876\u001b[0m \u001b[39m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[1;32m    877\u001b[0m \u001b[39m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[1;32m    878\u001b[0m first_test_score \u001b[39m=\u001b[39m all_out[\u001b[39m0\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mtest_scores\u001b[39m\u001b[39m\"\u001b[39m]\n",
            "File \u001b[0;32m~/opt/anaconda3/envs/tensorflow/lib/python3.9/site-packages/sklearn/model_selection/_search.py:1388\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1386\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_run_search\u001b[39m(\u001b[39mself\u001b[39m, evaluate_candidates):\n\u001b[1;32m   1387\u001b[0m     \u001b[39m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1388\u001b[0m     evaluate_candidates(ParameterGrid(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparam_grid))\n",
            "File \u001b[0;32m~/opt/anaconda3/envs/tensorflow/lib/python3.9/site-packages/sklearn/model_selection/_search.py:821\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    813\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    814\u001b[0m     \u001b[39mprint\u001b[39m(\n\u001b[1;32m    815\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mFitting \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m folds for each of \u001b[39m\u001b[39m{1}\u001b[39;00m\u001b[39m candidates,\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    816\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m totalling \u001b[39m\u001b[39m{2}\u001b[39;00m\u001b[39m fits\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m    817\u001b[0m             n_splits, n_candidates, n_candidates \u001b[39m*\u001b[39m n_splits\n\u001b[1;32m    818\u001b[0m         )\n\u001b[1;32m    819\u001b[0m     )\n\u001b[0;32m--> 821\u001b[0m out \u001b[39m=\u001b[39m parallel(\n\u001b[1;32m    822\u001b[0m     delayed(_fit_and_score)(\n\u001b[1;32m    823\u001b[0m         clone(base_estimator),\n\u001b[1;32m    824\u001b[0m         X,\n\u001b[1;32m    825\u001b[0m         y,\n\u001b[1;32m    826\u001b[0m         train\u001b[39m=\u001b[39;49mtrain,\n\u001b[1;32m    827\u001b[0m         test\u001b[39m=\u001b[39;49mtest,\n\u001b[1;32m    828\u001b[0m         parameters\u001b[39m=\u001b[39;49mparameters,\n\u001b[1;32m    829\u001b[0m         split_progress\u001b[39m=\u001b[39;49m(split_idx, n_splits),\n\u001b[1;32m    830\u001b[0m         candidate_progress\u001b[39m=\u001b[39;49m(cand_idx, n_candidates),\n\u001b[1;32m    831\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_and_score_kwargs,\n\u001b[1;32m    832\u001b[0m     )\n\u001b[1;32m    833\u001b[0m     \u001b[39mfor\u001b[39;49;00m (cand_idx, parameters), (split_idx, (train, test)) \u001b[39min\u001b[39;49;00m product(\n\u001b[1;32m    834\u001b[0m         \u001b[39menumerate\u001b[39;49m(candidate_params), \u001b[39menumerate\u001b[39;49m(cv\u001b[39m.\u001b[39;49msplit(X, y, groups))\n\u001b[1;32m    835\u001b[0m     )\n\u001b[1;32m    836\u001b[0m )\n\u001b[1;32m    838\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(out) \u001b[39m<\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m    839\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    840\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mNo fits were performed. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    841\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mWas the CV iterator empty? \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    842\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mWere there no candidates?\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    843\u001b[0m     )\n",
            "File \u001b[0;32m~/opt/anaconda3/envs/tensorflow/lib/python3.9/site-packages/sklearn/utils/parallel.py:63\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     58\u001b[0m config \u001b[39m=\u001b[39m get_config()\n\u001b[1;32m     59\u001b[0m iterable_with_config \u001b[39m=\u001b[39m (\n\u001b[1;32m     60\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     61\u001b[0m     \u001b[39mfor\u001b[39;00m delayed_func, args, kwargs \u001b[39min\u001b[39;00m iterable\n\u001b[1;32m     62\u001b[0m )\n\u001b[0;32m---> 63\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__call__\u001b[39;49m(iterable_with_config)\n",
            "File \u001b[0;32m~/opt/anaconda3/envs/tensorflow/lib/python3.9/site-packages/joblib/parallel.py:1051\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1048\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdispatch_one_batch(iterator):\n\u001b[1;32m   1049\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterating \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_original_iterator \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m-> 1051\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdispatch_one_batch(iterator):\n\u001b[1;32m   1052\u001b[0m     \u001b[39mpass\u001b[39;00m\n\u001b[1;32m   1054\u001b[0m \u001b[39mif\u001b[39;00m pre_dispatch \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mall\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mor\u001b[39;00m n_jobs \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m   1055\u001b[0m     \u001b[39m# The iterable was consumed all at once by the above for loop.\u001b[39;00m\n\u001b[1;32m   1056\u001b[0m     \u001b[39m# No need to wait for async callbacks to trigger to\u001b[39;00m\n\u001b[1;32m   1057\u001b[0m     \u001b[39m# consumption.\u001b[39;00m\n",
            "File \u001b[0;32m~/opt/anaconda3/envs/tensorflow/lib/python3.9/site-packages/joblib/parallel.py:864\u001b[0m, in \u001b[0;36mParallel.dispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    862\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m    863\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 864\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dispatch(tasks)\n\u001b[1;32m    865\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mTrue\u001b[39;00m\n",
            "File \u001b[0;32m~/opt/anaconda3/envs/tensorflow/lib/python3.9/site-packages/joblib/parallel.py:782\u001b[0m, in \u001b[0;36mParallel._dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    780\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[1;32m    781\u001b[0m     job_idx \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs)\n\u001b[0;32m--> 782\u001b[0m     job \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_backend\u001b[39m.\u001b[39;49mapply_async(batch, callback\u001b[39m=\u001b[39;49mcb)\n\u001b[1;32m    783\u001b[0m     \u001b[39m# A job can complete so quickly than its callback is\u001b[39;00m\n\u001b[1;32m    784\u001b[0m     \u001b[39m# called before we get here, causing self._jobs to\u001b[39;00m\n\u001b[1;32m    785\u001b[0m     \u001b[39m# grow. To ensure correct results ordering, .insert is\u001b[39;00m\n\u001b[1;32m    786\u001b[0m     \u001b[39m# used (rather than .append) in the following line\u001b[39;00m\n\u001b[1;32m    787\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs\u001b[39m.\u001b[39minsert(job_idx, job)\n",
            "File \u001b[0;32m~/opt/anaconda3/envs/tensorflow/lib/python3.9/site-packages/joblib/_parallel_backends.py:208\u001b[0m, in \u001b[0;36mSequentialBackend.apply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply_async\u001b[39m(\u001b[39mself\u001b[39m, func, callback\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    207\u001b[0m     \u001b[39m\"\"\"Schedule a func to be run\"\"\"\u001b[39;00m\n\u001b[0;32m--> 208\u001b[0m     result \u001b[39m=\u001b[39m ImmediateResult(func)\n\u001b[1;32m    209\u001b[0m     \u001b[39mif\u001b[39;00m callback:\n\u001b[1;32m    210\u001b[0m         callback(result)\n",
            "File \u001b[0;32m~/opt/anaconda3/envs/tensorflow/lib/python3.9/site-packages/joblib/_parallel_backends.py:572\u001b[0m, in \u001b[0;36mImmediateResult.__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    569\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, batch):\n\u001b[1;32m    570\u001b[0m     \u001b[39m# Don't delay the application, to avoid keeping the input\u001b[39;00m\n\u001b[1;32m    571\u001b[0m     \u001b[39m# arguments in memory\u001b[39;00m\n\u001b[0;32m--> 572\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresults \u001b[39m=\u001b[39m batch()\n",
            "File \u001b[0;32m~/opt/anaconda3/envs/tensorflow/lib/python3.9/site-packages/joblib/parallel.py:263\u001b[0m, in \u001b[0;36mBatchedCalls.__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    259\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    260\u001b[0m     \u001b[39m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[1;32m    261\u001b[0m     \u001b[39m# change the default number of processes to -1\u001b[39;00m\n\u001b[1;32m    262\u001b[0m     \u001b[39mwith\u001b[39;00m parallel_backend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, n_jobs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_jobs):\n\u001b[0;32m--> 263\u001b[0m         \u001b[39mreturn\u001b[39;00m [func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    264\u001b[0m                 \u001b[39mfor\u001b[39;00m func, args, kwargs \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems]\n",
            "File \u001b[0;32m~/opt/anaconda3/envs/tensorflow/lib/python3.9/site-packages/joblib/parallel.py:263\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    259\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    260\u001b[0m     \u001b[39m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[1;32m    261\u001b[0m     \u001b[39m# change the default number of processes to -1\u001b[39;00m\n\u001b[1;32m    262\u001b[0m     \u001b[39mwith\u001b[39;00m parallel_backend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, n_jobs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_jobs):\n\u001b[0;32m--> 263\u001b[0m         \u001b[39mreturn\u001b[39;00m [func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    264\u001b[0m                 \u001b[39mfor\u001b[39;00m func, args, kwargs \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems]\n",
            "File \u001b[0;32m~/opt/anaconda3/envs/tensorflow/lib/python3.9/site-packages/sklearn/utils/parallel.py:123\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    121\u001b[0m     config \u001b[39m=\u001b[39m {}\n\u001b[1;32m    122\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mconfig):\n\u001b[0;32m--> 123\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfunction(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
            "File \u001b[0;32m~/opt/anaconda3/envs/tensorflow/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:686\u001b[0m, in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[1;32m    684\u001b[0m         estimator\u001b[39m.\u001b[39mfit(X_train, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\n\u001b[1;32m    685\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 686\u001b[0m         estimator\u001b[39m.\u001b[39;49mfit(X_train, y_train, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_params)\n\u001b[1;32m    688\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n\u001b[1;32m    689\u001b[0m     \u001b[39m# Note fit time as time until error\u001b[39;00m\n\u001b[1;32m    690\u001b[0m     fit_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m start_time\n",
            "File \u001b[0;32m~/opt/anaconda3/envs/tensorflow/lib/python3.9/site-packages/sklearn/pipeline.py:405\u001b[0m, in \u001b[0;36mPipeline.fit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    403\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_final_estimator \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mpassthrough\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    404\u001b[0m         fit_params_last_step \u001b[39m=\u001b[39m fit_params_steps[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msteps[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m][\u001b[39m0\u001b[39m]]\n\u001b[0;32m--> 405\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_final_estimator\u001b[39m.\u001b[39;49mfit(Xt, y, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_params_last_step)\n\u001b[1;32m    407\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
            "File \u001b[0;32m~/opt/anaconda3/envs/tensorflow/lib/python3.9/site-packages/sklearn/tree/_classes.py:889\u001b[0m, in \u001b[0;36mDecisionTreeClassifier.fit\u001b[0;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[1;32m    859\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfit\u001b[39m(\u001b[39mself\u001b[39m, X, y, sample_weight\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, check_input\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[1;32m    860\u001b[0m     \u001b[39m\"\"\"Build a decision tree classifier from the training set (X, y).\u001b[39;00m\n\u001b[1;32m    861\u001b[0m \n\u001b[1;32m    862\u001b[0m \u001b[39m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    886\u001b[0m \u001b[39m        Fitted estimator.\u001b[39;00m\n\u001b[1;32m    887\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 889\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mfit(\n\u001b[1;32m    890\u001b[0m         X,\n\u001b[1;32m    891\u001b[0m         y,\n\u001b[1;32m    892\u001b[0m         sample_weight\u001b[39m=\u001b[39;49msample_weight,\n\u001b[1;32m    893\u001b[0m         check_input\u001b[39m=\u001b[39;49mcheck_input,\n\u001b[1;32m    894\u001b[0m     )\n\u001b[1;32m    895\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
            "File \u001b[0;32m~/opt/anaconda3/envs/tensorflow/lib/python3.9/site-packages/sklearn/tree/_classes.py:379\u001b[0m, in \u001b[0;36mBaseDecisionTree.fit\u001b[0;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[1;32m    368\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    369\u001b[0m     builder \u001b[39m=\u001b[39m BestFirstTreeBuilder(\n\u001b[1;32m    370\u001b[0m         splitter,\n\u001b[1;32m    371\u001b[0m         min_samples_split,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    376\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmin_impurity_decrease,\n\u001b[1;32m    377\u001b[0m     )\n\u001b[0;32m--> 379\u001b[0m builder\u001b[39m.\u001b[39;49mbuild(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtree_, X, y, sample_weight)\n\u001b[1;32m    381\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_outputs_ \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m \u001b[39mand\u001b[39;00m is_classifier(\u001b[39mself\u001b[39m):\n\u001b[1;32m    382\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_classes_ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_classes_[\u001b[39m0\u001b[39m]\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# YOUR CODE HERE\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "classifierDT = Pipeline([\n",
        "    ('dr', PCA()),\n",
        "    (\"predictor\", DecisionTreeClassifier())\n",
        "])\n",
        "\n",
        "dt_param_grid = {\n",
        "    \"dr__n_components\": [150, 210, 270, 327],\n",
        "    \"predictor__max_depth\": [10, 20, 30, 40, None],\n",
        "    \"predictor__min_samples_split\": [2, 5, 10, 15],\n",
        "    \"predictor__min_samples_leaf\": [1, 2, 4, 6]\n",
        "}\n",
        "\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "dt_gs = GridSearchCV(classifierDT, dt_param_grid, scoring=\"accuracy\")\n",
        "dt_gs.fit(train_X, train_y)\n",
        "\n",
        "dt_gs.best_params_, dt_gs.best_score_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vh-W8kc74SvZ"
      },
      "source": [
        "<font color=\"red\">**My comment:**</font> Evaluate classifier decition tree fitted after grid search of hyperparameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ekcTp2jN4Sva",
        "outputId": "e86ae0a6-4255-4d5d-9fa6-e2cbad02d2bb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CV Train accuracy:  0.8690709883588223\n",
            "CV Test accuracy:  0.816360153256705\n",
            "Pred. Train accuracy:  0.9946682464454977\n",
            "Pred. Test accuracy:  0.8908839779005525\n"
          ]
        }
      ],
      "source": [
        "classifierDT.set_params(**dt_gs.best_params_)\n",
        "classifierDT.fit(train_X, train_y)\n",
        "\n",
        "from sklearn.model_selection import cross_val_score\n",
        "train_acc = np.mean(cross_val_score(classifierDT, train_X, train_y, scoring=\"accuracy\"))\n",
        "test_acc = np.mean(cross_val_score(classifierDT, test_X, test_y, scoring=\"accuracy\"))\n",
        "print(\"CV Train accuracy: \", train_acc)\n",
        "print(\"CV Test accuracy: \", test_acc)\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "predictions = classifierDT.predict(train_X)\n",
        "train_acc = accuracy_score(predictions, train_y)\n",
        "\n",
        "predictions = classifierDT.predict(test_X)\n",
        "test_acc = accuracy_score(predictions, test_y)\n",
        "print(\"Pred. Train accuracy: \", train_acc)\n",
        "print(\"Pred. Test accuracy: \", test_acc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PjL3kyI_4Sva"
      },
      "source": [
        "<font color=\"red\">**My comment:**</font>\n",
        "\n",
        "Grid search found optimal:\n",
        "\n",
        "({'dr__n_components': 150, \\\n",
        "  'predictor__max_depth': None, \\\n",
        "  'predictor__min_samples_leaf': 1, \\\n",
        "  'predictor__min_samples_split': 2}, \\\n",
        " 0.8803223710779063)\n",
        "\n",
        " Decision tree set to these hyperparameters values indeed produce very high training accuracy of 0.9947. However, the test accuracy has degraded to 0.8909 from 0.9848 previously. The decision tree with these grid search hyperparameters has been probably overfitted.\n",
        "\n",
        " I had in the previous part experimented with standardising the features using:\n",
        "\n",
        "scaler = StandardScaler() \\\n",
        "train_X = scaler.fit_transform(train_X) \\\n",
        "test_X = scaler.transform(test_X)\n",
        "\n",
        "I did not find that this has any substantial effect on the train and test accuracies."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FYoMg0EZIrNd"
      },
      "source": [
        "## New classifier (10 Marks)\n",
        "\n",
        "Replicate the previous task for a classifier different than K-NN and decision trees. Briefly describe your choice.\n",
        "Try to create the best model for the given dataset.\n",
        "\n",
        "\n",
        "Save your best model into your github. And create a single code cell that loads it and evaluate it on the following test dataset:\n",
        "https://github.com/andvise/DataAnalyticsDatasets/blob/main/test_dataset.csv\n",
        "\n",
        "This link currently contains a sample of the training set. The real test set will be released after the submission. I should be able to run the code cell independently, load all the libraries you need as well."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yEPl98z14Svb"
      },
      "source": [
        "<font color=\"red\">**My comment:**</font>\n",
        "\n",
        "Using experience I gained from the Deep Learning module this semester, I set up a simple neural network (NN) for the current classification problem. The NN has 5 dense (fully connected) layers, including the input and output layers. Each layer has a drop out rate of 0.35 to allow the training not to overfit to the training data. The training is very fast even without using GPU on Colab. Training accuracy = 0.9995. In the final (next) part, I test the model using the test data provided."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QRJXrY2hI32F",
        "outputId": "6b477787-26af-4cc7-ee23-8e79a5b378e4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "61/61 [==============================] - 3s 11ms/step - loss: 1.7542 - accuracy: 0.4749 - val_loss: 0.8961 - val_accuracy: 0.8841\n",
            "Epoch 2/100\n",
            "61/61 [==============================] - 0s 6ms/step - loss: 0.8599 - accuracy: 0.7745 - val_loss: 0.3847 - val_accuracy: 0.9234\n",
            "Epoch 3/100\n",
            "61/61 [==============================] - 0s 6ms/step - loss: 0.6029 - accuracy: 0.8372 - val_loss: 0.2232 - val_accuracy: 0.9607\n",
            "Epoch 4/100\n",
            "61/61 [==============================] - 0s 6ms/step - loss: 0.5070 - accuracy: 0.8616 - val_loss: 0.1552 - val_accuracy: 0.9793\n",
            "Epoch 5/100\n",
            "61/61 [==============================] - 0s 6ms/step - loss: 0.3943 - accuracy: 0.8968 - val_loss: 0.1196 - val_accuracy: 0.9814\n",
            "Epoch 6/100\n",
            "61/61 [==============================] - 0s 6ms/step - loss: 0.3237 - accuracy: 0.9119 - val_loss: 0.0895 - val_accuracy: 0.9814\n",
            "Epoch 7/100\n",
            "61/61 [==============================] - 0s 7ms/step - loss: 0.2883 - accuracy: 0.9248 - val_loss: 0.0867 - val_accuracy: 0.9855\n",
            "Epoch 8/100\n",
            "61/61 [==============================] - 0s 6ms/step - loss: 0.2429 - accuracy: 0.9285 - val_loss: 0.0739 - val_accuracy: 0.9876\n",
            "Epoch 9/100\n",
            "61/61 [==============================] - 0s 6ms/step - loss: 0.2190 - accuracy: 0.9388 - val_loss: 0.0802 - val_accuracy: 0.9814\n",
            "Epoch 10/100\n",
            "61/61 [==============================] - 0s 6ms/step - loss: 0.1893 - accuracy: 0.9482 - val_loss: 0.0687 - val_accuracy: 0.9876\n",
            "Epoch 11/100\n",
            "61/61 [==============================] - 0s 6ms/step - loss: 0.1762 - accuracy: 0.9487 - val_loss: 0.0518 - val_accuracy: 0.9876\n",
            "Epoch 12/100\n",
            "61/61 [==============================] - 0s 6ms/step - loss: 0.1666 - accuracy: 0.9497 - val_loss: 0.0478 - val_accuracy: 0.9896\n",
            "Epoch 13/100\n",
            "61/61 [==============================] - 0s 6ms/step - loss: 0.1493 - accuracy: 0.9544 - val_loss: 0.0476 - val_accuracy: 0.9896\n",
            "Epoch 14/100\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 0.1366 - accuracy: 0.9596 - val_loss: 0.0433 - val_accuracy: 0.9855\n",
            "Epoch 15/100\n",
            "61/61 [==============================] - 0s 6ms/step - loss: 0.1317 - accuracy: 0.9616 - val_loss: 0.0576 - val_accuracy: 0.9752\n",
            "Epoch 16/100\n",
            "61/61 [==============================] - 0s 6ms/step - loss: 0.1393 - accuracy: 0.9596 - val_loss: 0.0324 - val_accuracy: 0.9917\n",
            "Epoch 17/100\n",
            "61/61 [==============================] - 0s 6ms/step - loss: 0.1315 - accuracy: 0.9570 - val_loss: 0.0408 - val_accuracy: 0.9917\n",
            "Epoch 18/100\n",
            "61/61 [==============================] - 0s 6ms/step - loss: 0.1068 - accuracy: 0.9705 - val_loss: 0.0278 - val_accuracy: 0.9938\n",
            "Epoch 19/100\n",
            "61/61 [==============================] - 0s 6ms/step - loss: 0.0969 - accuracy: 0.9725 - val_loss: 0.0264 - val_accuracy: 0.9917\n",
            "Epoch 20/100\n",
            "61/61 [==============================] - 0s 6ms/step - loss: 0.1113 - accuracy: 0.9684 - val_loss: 0.0200 - val_accuracy: 0.9938\n",
            "Epoch 21/100\n",
            "61/61 [==============================] - 0s 7ms/step - loss: 0.1000 - accuracy: 0.9673 - val_loss: 0.0187 - val_accuracy: 0.9917\n",
            "Epoch 22/100\n",
            "61/61 [==============================] - 0s 7ms/step - loss: 0.0924 - accuracy: 0.9730 - val_loss: 0.0202 - val_accuracy: 0.9917\n",
            "Epoch 23/100\n",
            "61/61 [==============================] - 0s 6ms/step - loss: 0.0853 - accuracy: 0.9736 - val_loss: 0.0177 - val_accuracy: 0.9938\n",
            "Epoch 24/100\n",
            "61/61 [==============================] - 0s 6ms/step - loss: 0.0808 - accuracy: 0.9793 - val_loss: 0.0163 - val_accuracy: 0.9938\n",
            "Epoch 25/100\n",
            "61/61 [==============================] - 0s 7ms/step - loss: 0.0743 - accuracy: 0.9834 - val_loss: 0.0187 - val_accuracy: 0.9938\n",
            "Epoch 26/100\n",
            "61/61 [==============================] - 0s 7ms/step - loss: 0.0859 - accuracy: 0.9725 - val_loss: 0.0230 - val_accuracy: 0.9896\n",
            "Epoch 27/100\n",
            "61/61 [==============================] - 0s 7ms/step - loss: 0.0718 - accuracy: 0.9793 - val_loss: 0.0203 - val_accuracy: 0.9938\n",
            "Epoch 28/100\n",
            "61/61 [==============================] - 0s 6ms/step - loss: 0.0864 - accuracy: 0.9782 - val_loss: 0.0227 - val_accuracy: 0.9938\n",
            "Epoch 29/100\n",
            "61/61 [==============================] - 0s 6ms/step - loss: 0.0761 - accuracy: 0.9819 - val_loss: 0.0832 - val_accuracy: 0.9689\n",
            "Epoch 30/100\n",
            "61/61 [==============================] - 0s 7ms/step - loss: 0.0828 - accuracy: 0.9782 - val_loss: 0.0202 - val_accuracy: 0.9938\n",
            "Epoch 31/100\n",
            "61/61 [==============================] - 0s 8ms/step - loss: 0.0627 - accuracy: 0.9834 - val_loss: 0.0192 - val_accuracy: 0.9959\n",
            "Epoch 32/100\n",
            "61/61 [==============================] - 0s 7ms/step - loss: 0.0789 - accuracy: 0.9705 - val_loss: 0.0168 - val_accuracy: 0.9938\n",
            "Epoch 33/100\n",
            "61/61 [==============================] - 0s 7ms/step - loss: 0.0680 - accuracy: 0.9803 - val_loss: 0.0353 - val_accuracy: 0.9917\n",
            "Epoch 34/100\n",
            "61/61 [==============================] - 0s 6ms/step - loss: 0.0639 - accuracy: 0.9834 - val_loss: 0.0212 - val_accuracy: 0.9938\n",
            "Epoch 35/100\n",
            "61/61 [==============================] - 0s 7ms/step - loss: 0.0775 - accuracy: 0.9798 - val_loss: 0.0462 - val_accuracy: 0.9834\n",
            "Epoch 36/100\n",
            "61/61 [==============================] - 0s 7ms/step - loss: 0.0833 - accuracy: 0.9746 - val_loss: 0.0362 - val_accuracy: 0.9896\n",
            "Epoch 37/100\n",
            "61/61 [==============================] - 0s 7ms/step - loss: 0.0852 - accuracy: 0.9793 - val_loss: 0.0343 - val_accuracy: 0.9938\n",
            "Epoch 38/100\n",
            "61/61 [==============================] - 0s 6ms/step - loss: 0.0706 - accuracy: 0.9839 - val_loss: 0.0257 - val_accuracy: 0.9938\n",
            "Epoch 39/100\n",
            "61/61 [==============================] - 0s 7ms/step - loss: 0.0543 - accuracy: 0.9813 - val_loss: 0.0222 - val_accuracy: 0.9959\n",
            "Epoch 40/100\n",
            "61/61 [==============================] - 0s 7ms/step - loss: 0.0456 - accuracy: 0.9896 - val_loss: 0.0127 - val_accuracy: 0.9979\n",
            "Epoch 41/100\n",
            "61/61 [==============================] - 0s 6ms/step - loss: 0.0608 - accuracy: 0.9829 - val_loss: 0.0183 - val_accuracy: 0.9959\n",
            "Epoch 42/100\n",
            "61/61 [==============================] - 0s 7ms/step - loss: 0.0553 - accuracy: 0.9855 - val_loss: 0.0177 - val_accuracy: 0.9959\n",
            "Epoch 43/100\n",
            "61/61 [==============================] - 0s 6ms/step - loss: 0.0571 - accuracy: 0.9819 - val_loss: 0.0381 - val_accuracy: 0.9896\n",
            "Epoch 44/100\n",
            "61/61 [==============================] - 0s 7ms/step - loss: 0.0466 - accuracy: 0.9850 - val_loss: 0.0534 - val_accuracy: 0.9917\n",
            "Epoch 45/100\n",
            "61/61 [==============================] - 0s 6ms/step - loss: 0.0537 - accuracy: 0.9834 - val_loss: 0.0162 - val_accuracy: 0.9959\n",
            "Epoch 46/100\n",
            "61/61 [==============================] - 0s 7ms/step - loss: 0.0499 - accuracy: 0.9881 - val_loss: 0.0198 - val_accuracy: 0.9959\n",
            "Epoch 47/100\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 0.0645 - accuracy: 0.9808 - val_loss: 0.0362 - val_accuracy: 0.9917\n",
            "Epoch 48/100\n",
            "61/61 [==============================] - 0s 6ms/step - loss: 0.0473 - accuracy: 0.9834 - val_loss: 0.0123 - val_accuracy: 0.9959\n",
            "Epoch 49/100\n",
            "61/61 [==============================] - 0s 6ms/step - loss: 0.0504 - accuracy: 0.9844 - val_loss: 0.0143 - val_accuracy: 0.9959\n",
            "Epoch 50/100\n",
            "61/61 [==============================] - 0s 6ms/step - loss: 0.0645 - accuracy: 0.9772 - val_loss: 0.0155 - val_accuracy: 0.9938\n",
            "Epoch 51/100\n",
            "61/61 [==============================] - 1s 10ms/step - loss: 0.0565 - accuracy: 0.9824 - val_loss: 0.0236 - val_accuracy: 0.9938\n",
            "Epoch 52/100\n",
            "61/61 [==============================] - 1s 8ms/step - loss: 0.0375 - accuracy: 0.9896 - val_loss: 0.0320 - val_accuracy: 0.9938\n",
            "Epoch 53/100\n",
            "61/61 [==============================] - 0s 7ms/step - loss: 0.0352 - accuracy: 0.9907 - val_loss: 0.0212 - val_accuracy: 0.9959\n",
            "Epoch 54/100\n",
            "61/61 [==============================] - 1s 12ms/step - loss: 0.0549 - accuracy: 0.9855 - val_loss: 0.0225 - val_accuracy: 0.9896\n",
            "Epoch 55/100\n",
            "61/61 [==============================] - 1s 13ms/step - loss: 0.0387 - accuracy: 0.9891 - val_loss: 0.0203 - val_accuracy: 0.9959\n",
            "Epoch 56/100\n",
            "61/61 [==============================] - 1s 15ms/step - loss: 0.0611 - accuracy: 0.9844 - val_loss: 0.0146 - val_accuracy: 0.9959\n",
            "Epoch 57/100\n",
            "61/61 [==============================] - 1s 10ms/step - loss: 0.0420 - accuracy: 0.9896 - val_loss: 0.0218 - val_accuracy: 0.9959\n",
            "Epoch 58/100\n",
            "61/61 [==============================] - 1s 10ms/step - loss: 0.0355 - accuracy: 0.9886 - val_loss: 0.0291 - val_accuracy: 0.9938\n",
            "Epoch 59/100\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 0.0360 - accuracy: 0.9891 - val_loss: 0.0312 - val_accuracy: 0.9938\n",
            "Epoch 60/100\n",
            "61/61 [==============================] - 0s 8ms/step - loss: 0.0405 - accuracy: 0.9870 - val_loss: 0.0525 - val_accuracy: 0.9814\n",
            "Epoch 61/100\n",
            "61/61 [==============================] - 0s 8ms/step - loss: 0.0599 - accuracy: 0.9824 - val_loss: 0.0210 - val_accuracy: 0.9917\n",
            "Epoch 62/100\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 0.0424 - accuracy: 0.9886 - val_loss: 0.0237 - val_accuracy: 0.9917\n",
            "Epoch 63/100\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 0.0353 - accuracy: 0.9896 - val_loss: 0.0147 - val_accuracy: 0.9959\n",
            "Epoch 64/100\n",
            "61/61 [==============================] - 0s 8ms/step - loss: 0.0402 - accuracy: 0.9870 - val_loss: 0.0148 - val_accuracy: 0.9979\n",
            "Epoch 65/100\n",
            "61/61 [==============================] - 0s 8ms/step - loss: 0.0453 - accuracy: 0.9865 - val_loss: 0.0161 - val_accuracy: 0.9917\n",
            "Epoch 66/100\n",
            "61/61 [==============================] - 1s 11ms/step - loss: 0.0543 - accuracy: 0.9870 - val_loss: 0.0222 - val_accuracy: 0.9938\n",
            "Epoch 67/100\n",
            "61/61 [==============================] - 0s 8ms/step - loss: 0.0315 - accuracy: 0.9891 - val_loss: 0.0376 - val_accuracy: 0.9917\n",
            "Epoch 68/100\n",
            "61/61 [==============================] - 0s 8ms/step - loss: 0.0264 - accuracy: 0.9912 - val_loss: 0.0282 - val_accuracy: 0.9938\n",
            "Epoch 69/100\n",
            "61/61 [==============================] - 0s 8ms/step - loss: 0.0370 - accuracy: 0.9912 - val_loss: 0.0235 - val_accuracy: 0.9938\n",
            "Epoch 70/100\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 0.0404 - accuracy: 0.9886 - val_loss: 0.0219 - val_accuracy: 0.9917\n",
            "Epoch 71/100\n",
            "61/61 [==============================] - 0s 7ms/step - loss: 0.0468 - accuracy: 0.9855 - val_loss: 0.0202 - val_accuracy: 0.9938\n",
            "Epoch 72/100\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 0.0450 - accuracy: 0.9881 - val_loss: 0.0428 - val_accuracy: 0.9896\n",
            "Epoch 73/100\n",
            "61/61 [==============================] - 0s 7ms/step - loss: 0.0305 - accuracy: 0.9902 - val_loss: 0.0172 - val_accuracy: 0.9959\n",
            "Epoch 74/100\n",
            "61/61 [==============================] - 0s 8ms/step - loss: 0.0378 - accuracy: 0.9896 - val_loss: 0.0137 - val_accuracy: 0.9959\n",
            "Epoch 75/100\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 0.0568 - accuracy: 0.9829 - val_loss: 0.0107 - val_accuracy: 0.9979\n",
            "Epoch 76/100\n",
            "61/61 [==============================] - 0s 8ms/step - loss: 0.0367 - accuracy: 0.9902 - val_loss: 0.0218 - val_accuracy: 0.9959\n",
            "Epoch 77/100\n",
            "61/61 [==============================] - 0s 7ms/step - loss: 0.0400 - accuracy: 0.9896 - val_loss: 0.0195 - val_accuracy: 0.9938\n",
            "Epoch 78/100\n",
            "61/61 [==============================] - 0s 7ms/step - loss: 0.0349 - accuracy: 0.9896 - val_loss: 0.0218 - val_accuracy: 0.9959\n",
            "Epoch 79/100\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 0.0246 - accuracy: 0.9902 - val_loss: 0.0246 - val_accuracy: 0.9938\n",
            "Epoch 80/100\n",
            "61/61 [==============================] - 1s 10ms/step - loss: 0.0318 - accuracy: 0.9922 - val_loss: 0.0237 - val_accuracy: 0.9959\n",
            "Epoch 81/100\n",
            "61/61 [==============================] - 1s 14ms/step - loss: 0.0237 - accuracy: 0.9943 - val_loss: 0.0186 - val_accuracy: 0.9938\n",
            "Epoch 82/100\n",
            "61/61 [==============================] - 1s 12ms/step - loss: 0.0364 - accuracy: 0.9922 - val_loss: 0.0403 - val_accuracy: 0.9917\n",
            "Epoch 83/100\n",
            "61/61 [==============================] - 1s 13ms/step - loss: 0.0167 - accuracy: 0.9953 - val_loss: 0.0202 - val_accuracy: 0.9917\n",
            "Epoch 84/100\n",
            "61/61 [==============================] - 1s 12ms/step - loss: 0.0261 - accuracy: 0.9917 - val_loss: 0.0163 - val_accuracy: 0.9959\n",
            "Epoch 85/100\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 0.0447 - accuracy: 0.9870 - val_loss: 0.0095 - val_accuracy: 0.9959\n",
            "Epoch 86/100\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 0.0425 - accuracy: 0.9896 - val_loss: 0.0229 - val_accuracy: 0.9959\n",
            "Epoch 87/100\n",
            "61/61 [==============================] - 0s 8ms/step - loss: 0.0365 - accuracy: 0.9886 - val_loss: 0.0165 - val_accuracy: 0.9959\n",
            "Epoch 88/100\n",
            "61/61 [==============================] - 0s 8ms/step - loss: 0.0353 - accuracy: 0.9912 - val_loss: 0.0148 - val_accuracy: 0.9979\n",
            "Epoch 89/100\n",
            "61/61 [==============================] - 1s 12ms/step - loss: 0.0398 - accuracy: 0.9912 - val_loss: 0.0082 - val_accuracy: 0.9979\n",
            "Epoch 90/100\n",
            "61/61 [==============================] - 0s 8ms/step - loss: 0.0446 - accuracy: 0.9855 - val_loss: 0.0213 - val_accuracy: 0.9959\n",
            "Epoch 91/100\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 0.0295 - accuracy: 0.9917 - val_loss: 0.0221 - val_accuracy: 0.9959\n",
            "Epoch 92/100\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 0.0316 - accuracy: 0.9896 - val_loss: 0.0175 - val_accuracy: 0.9938\n",
            "Epoch 93/100\n",
            "61/61 [==============================] - 0s 8ms/step - loss: 0.0232 - accuracy: 0.9953 - val_loss: 0.0459 - val_accuracy: 0.9834\n",
            "Epoch 94/100\n",
            "61/61 [==============================] - 0s 8ms/step - loss: 0.0295 - accuracy: 0.9927 - val_loss: 0.0249 - val_accuracy: 0.9959\n",
            "Epoch 95/100\n",
            "61/61 [==============================] - 0s 8ms/step - loss: 0.0307 - accuracy: 0.9927 - val_loss: 0.0113 - val_accuracy: 0.9959\n",
            "Epoch 96/100\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 0.0303 - accuracy: 0.9922 - val_loss: 0.0176 - val_accuracy: 0.9959\n",
            "Epoch 97/100\n",
            "61/61 [==============================] - 1s 9ms/step - loss: 0.0151 - accuracy: 0.9964 - val_loss: 0.0212 - val_accuracy: 0.9959\n",
            "Epoch 98/100\n",
            "61/61 [==============================] - 0s 8ms/step - loss: 0.0269 - accuracy: 0.9948 - val_loss: 0.0364 - val_accuracy: 0.9938\n",
            "Epoch 99/100\n",
            "61/61 [==============================] - 0s 8ms/step - loss: 0.0186 - accuracy: 0.9953 - val_loss: 0.0286 - val_accuracy: 0.9938\n",
            "Epoch 100/100\n",
            "61/61 [==============================] - 1s 10ms/step - loss: 0.0232 - accuracy: 0.9938 - val_loss: 0.0287 - val_accuracy: 0.9917\n",
            "61/61 [==============================] - 0s 4ms/step - loss: 0.0065 - accuracy: 0.9974\n",
            "Training loss:  0.0064836470410227776\n",
            "Training accuracy:  0.9974079728126526\n"
          ]
        }
      ],
      "source": [
        "# YOUR CODE HERE\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, BatchNormalization, Dropout\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# Read in data.\n",
        "df = pd.read_csv(\"https://raw.githubusercontent.com/andvise/DataAnalyticsDatasets/main/train_dataset.csv\", index_col=0)\n",
        "\n",
        "X = df.iloc[:, :-1]\n",
        "y = df['target']\n",
        "\n",
        "# Data cleaning. Missing and infinity values.\n",
        "X = X.fillna(0)\n",
        "X = X.replace([np.inf, -np.inf], 0)\n",
        "\n",
        "# Split data into train and validation sets.\n",
        "train_X, valid_X, train_y, valid_y = train_test_split(X, y, train_size=0.8)\n",
        "\n",
        "# Preprocess the features and target class.\n",
        "scaler = StandardScaler()\n",
        "train_X = scaler.fit_transform(train_X)\n",
        "valid_X = scaler.transform(valid_X)\n",
        "\n",
        "encoder = LabelEncoder()\n",
        "train_y_encoded = encoder.fit_transform(train_y)\n",
        "valid_y_encoded = encoder.transform(valid_y)\n",
        "\n",
        "train_y_categorical = to_categorical(train_y_encoded)\n",
        "valid_y_categorical = to_categorical(valid_y_encoded)\n",
        "\n",
        "# For NN, we need to have validation and test sets.\n",
        "# Commented now. Use provided test data in the next section.\n",
        "# valid_X2, test_X, valid_y_categorical2, test_y_categorical = train_test_split(valid_X, valid_y_categorical, train_size=0.8)\n",
        "\n",
        "# Set up a dense neural network.\n",
        "model = Sequential()\n",
        "model.add(Dense(327, activation='relu', input_shape=(train_X.shape[1],)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.3))\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.3))\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.3))\n",
        "model.add(Dense(32, activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.3))\n",
        "model.add(Dense(14, activation='softmax'))\n",
        "\n",
        "# Compile the model.\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model.\n",
        "model.fit(train_X, train_y_categorical, epochs=100, batch_size=32, validation_data=(valid_X, valid_y_categorical))\n",
        "\n",
        "# Check the model training accuracy.\n",
        "# I use the provided test data for evaluation in the final part of this notebook.\n",
        "loss, accuracy = model.evaluate(train_X, train_y_categorical)\n",
        "print(\"Training loss: \", loss)\n",
        "print(\"Training accuracy: \", accuracy)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EW-2WCUv4Svc",
        "outputId": "1738122a-db2c-4eed-f9ae-63bd35b05898"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['mpk_model.joblib']"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from joblib import dump\n",
        "\n",
        "# Saving the trained NN model.\n",
        "# Only need to do once normally.\n",
        "dump(model, 'mpk_model.joblib')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q01BjiiCJTR4"
      },
      "source": [
        "# <font color=\"blue\">FOR GRADING ONLY</font>\n",
        "\n",
        "Save your best model into your github. And create a single code cell that loads it and evaluate it on the following test dataset:\n",
        "https://github.com/andvise/DataAnalyticsDatasets/blob/main/test_dataset.csv\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hVy8Toq24Svd"
      },
      "source": [
        "<font color=\"red\">**My comment:**</font> Using the test data provided, the trained NN produces a test accuracy of 1.0. I experimented a little bit with changing drop out rates of the neurons. In the end, I have chosen 0.35 dropout rate. I hope that this makes the model robust to unseen new data and will attain at least 0.99 accuracy.\n",
        "\n",
        "<font color=\"red\">**Note:**</font> Running this evaluation took not more than 20 seconds."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IWx4lyuQI929",
        "outputId": "bb6e98f8-1260-484e-8be8-89a84e13390c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "20/20 [==============================] - 0s 2ms/step - loss: 0.0019 - accuracy: 0.9984\n",
            "Test loss:  0.001877226517535746\n",
            "Test accuracy:  0.9984050989151001\n"
          ]
        }
      ],
      "source": [
        "from joblib import dump, load\n",
        "from io import BytesIO\n",
        "import requests\n",
        "\n",
        "# These packages are needed for preprocessing input data.\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# INSERT YOUR MODEL'S URL\n",
        "mLink = 'https://github.com/martin-king/dm_assign2/blob/main/mpk_model.joblib?raw=true'\n",
        "mfile = BytesIO(requests.get(mLink).content)\n",
        "model = load(mfile)\n",
        "\n",
        "# Input test data.\n",
        "df_test = pd.read_csv(\"https://raw.githubusercontent.com/andvise/DataAnalyticsDatasets/main/test_dataset.csv\", index_col=0)\n",
        "\n",
        "# YOUR CODE HERE\n",
        "X = df_test.iloc[:, :-1]\n",
        "y = df_test['target']\n",
        "\n",
        "# Data cleaning. Missing and infinity values.\n",
        "X = X.fillna(0)\n",
        "X = X.replace([np.inf, -np.inf], 0)\n",
        "\n",
        "# Preprocess the features and target class.\n",
        "# This part is critical for my model.\n",
        "scaler = StandardScaler()\n",
        "test_X = scaler.fit_transform(X)\n",
        "\n",
        "encoder = LabelEncoder()\n",
        "test_y_encoded = encoder.fit_transform(y)\n",
        "test_y_categorical = to_categorical(test_y_encoded)\n",
        "\n",
        "# Evaluate the model.\n",
        "loss, accuracy = model.evaluate(test_X, test_y_categorical)\n",
        "print(\"Test loss: \", loss)\n",
        "print(\"Test accuracy: \", accuracy)\n",
        "\n",
        "# Another way of doing the above.\n",
        "pred = model.predict(test_X)\n",
        "pred = np.argmax(pred, axis=1)\n",
        "true = np.argmax(test_y_categorical, axis=1)\n",
        "#from sklearn.metrics import accuracy_score\n",
        "#accuracy = accuracy_score(true, pred)\n",
        "#print(f'Test accuracy: {accuracy * 100:.2f}%')\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}